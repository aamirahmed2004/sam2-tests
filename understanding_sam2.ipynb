{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import argparse\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU, not recommended.\")\n",
    "\n",
    "# For efficiency\n",
    "if device.type == \"cuda\":\n",
    "    torch.autocast(\"cuda\", dtype = torch.bfloat16).__enter__()\n",
    "\n",
    "# Change this to the directory where you cloned this repo\n",
    "BASE_PATH = \"C:\\\\Users\\\\syeda\\\\OneDrive\\\\Desktop\\\\4th Year\\\\COSC419\\\\sam2-tests\"\n",
    "CHKPT_PATH = os.path.join(BASE_PATH, \"checkpoints\", \"sam2.1_hiera_large.pt\")\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, \"sam2\", \"configs\", \"sam2.1\", \"sam2.1_hiera_l.yaml\")\n",
    "FRAMES_PATH = os.path.join(BASE_PATH, \"sample_frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before loading model:\n",
      "  Allocated: 1.87 GB\n",
      "  Reserved: 2.15 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # remove lingering allocations\n",
    "model_allocated = torch.cuda.memory_allocated(device=device)\n",
    "model_reserved = torch.cuda.memory_reserved(device=device)\n",
    "\n",
    "print(f\"Memory before loading model:\")\n",
    "print(f\"  Allocated: {model_allocated / (1024 ** 3):.2f} GB\")\n",
    "print(f\"  Reserved: {model_reserved / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after loading model:\n",
      "  Allocated: 1.87 GB\n",
      "  Reserved: 2.92 GB\n"
     ]
    }
   ],
   "source": [
    "predictor = build_sam2_video_predictor(CONFIG_PATH, CHKPT_PATH, device=device)\n",
    "\n",
    "model_allocated = torch.cuda.memory_allocated(device=device)\n",
    "model_reserved = torch.cuda.memory_reserved(device=device)\n",
    "\n",
    "print(f\"Memory after loading model:\")\n",
    "print(f\"  Allocated: {model_allocated / (1024 ** 3):.2f} GB\")\n",
    "print(f\"  Reserved: {model_reserved / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG):   4%|▎         | 13/360 [00:00<00:26, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 360/360 [00:11<00:00, 30.94it/s]\n"
     ]
    }
   ],
   "source": [
    "inference_state = predictor.init_state(video_path=FRAMES_PATH, offload_video_to_cpu=True, async_loading_frames=True)\n",
    "predictor.reset_state(inference_state)\n",
    "print(inference_state[\"video_height\"], inference_state[\"video_width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 53\n"
     ]
    }
   ],
   "source": [
    "print(inference_state[\"video_height\"], inference_state[\"video_width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_dimensions(img_dir=FRAMES_PATH):\n",
    "    widths = []\n",
    "    heights = []\n",
    "    \n",
    "    # Iterate over files in the directory\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if filename.lower().endswith(\".jpg\"):\n",
    "            img_path = os.path.join(img_dir, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    widths.append(w)\n",
    "                    heights.append(h)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    \n",
    "    if not widths or not heights:\n",
    "        print(\"No images processed successfully.\")\n",
    "        return\n",
    "    \n",
    "    avg_width = np.mean(widths)\n",
    "    avg_height = np.mean(heights)\n",
    "    min_width = np.min(widths)\n",
    "    max_width = np.max(widths)\n",
    "    min_height = np.min(heights)\n",
    "    max_height = np.max(heights)\n",
    "    \n",
    "    print(\"First width:\", widths[0])\n",
    "    print(\"First height:\", heights[0])\n",
    "    print(\"Average width:\", avg_width)\n",
    "    print(\"Average height:\", avg_height)\n",
    "    print(\"Min width:\", min_width)\n",
    "    print(\"Min height:\", min_height)\n",
    "    print(\"Max width:\", max_width)\n",
    "    print(\"Max height:\", max_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First width: 53\n",
      "First height: 133\n",
      "Average width: 49.28888888888889\n",
      "Average height: 90.30555555555556\n",
      "Min width: 18\n",
      "Min height: 63\n",
      "Max width: 87\n",
      "Max height: 133\n"
     ]
    }
   ],
   "source": [
    "compute_image_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the \"inference state\" dictionary assigns its video_height and video_width according to the dimensions of the *first* frame - i.e. the model assumes all frames in the video are of the same dimension. So we need to pad all frames to the max height and width before feeding to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_height_and_width(frames_path):\n",
    "    widths = []\n",
    "    heights = []\n",
    "    \n",
    "    # Iterate over files in the directory\n",
    "    for filename in os.listdir(frames_path):\n",
    "        if filename.lower().endswith(\".jpg\"):\n",
    "            img_path = os.path.join(frames_path, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    widths.append(w)\n",
    "                    heights.append(h)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    \n",
    "    if not widths or not heights:\n",
    "        print(\"No images processed successfully.\")\n",
    "        return\n",
    "    \n",
    "    return np.max(heights), np.max(widths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_images(input_dir):\n",
    "\n",
    "    output_dir = input_dir + \"_resized\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    max_height, max_width = get_max_height_and_width(input_dir)\n",
    "    pad_amounts = None\n",
    "    \n",
    "    for idx, filename in enumerate(os.listdir(input_dir)):\n",
    "        if filename.lower().endswith(\".jpg\"):\n",
    "            img_path = os.path.join(input_dir, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    # Create a new image with the max dimensions and a black background\n",
    "                    new_img = Image.new(\"RGB\", (max_width, max_height), (0, 0, 0))\n",
    "                    # Calculate padding amounts\n",
    "                    pad_left = (max_width - img.width) // 2\n",
    "                    pad_top = (max_height - img.height) // 2\n",
    "                    # Paste the original image onto the center of the new image\n",
    "                    new_img.paste(img, (pad_left, pad_top))\n",
    "                    # Save the new image to the output directory\n",
    "                    new_img.save(os.path.join(output_dir, filename))\n",
    "                    \n",
    "                    # Track pad amounts for the first image only\n",
    "                    if idx == 0:\n",
    "                        pad_amounts = (pad_left, pad_top)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n",
    "    \n",
    "    return pad_amounts\n",
    "\n",
    "input_dir = FRAMES_PATH\n",
    "output_dir = input_dir + \"_resized\"\n",
    "first_bbox_pad = resize_and_pad_images(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad left: 26, Pad top: 31\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pad left: {first_bbox_pad[0]}, Pad top: {first_bbox_pad[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
