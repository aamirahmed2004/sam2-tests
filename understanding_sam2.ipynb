{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import argparse\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU, not recommended.\")\n",
    "\n",
    "# For efficiency\n",
    "if device.type == \"cuda\":\n",
    "    torch.autocast(\"cuda\", dtype = torch.bfloat16).__enter__()\n",
    "\n",
    "# Change this to the directory where you cloned this repo\n",
    "BASE_PATH = \"C:\\\\Users\\\\syeda\\\\OneDrive\\\\Desktop\\\\4th Year\\\\COSC419\\\\sam2-tests\"\n",
    "CHKPT_PATH = os.path.join(BASE_PATH, \"checkpoints\", \"sam2.1_hiera_large.pt\")\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, \"sam2\", \"configs\", \"sam2.1\", \"sam2.1_hiera_l.yaml\")\n",
    "FRAMES_PATH = os.path.join(BASE_PATH, \"sample_frames2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before loading model:\n",
      "  Allocated: 1.87 GB\n",
      "  Reserved: 2.15 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # remove lingering allocations\n",
    "model_allocated = torch.cuda.memory_allocated(device=device)\n",
    "model_reserved = torch.cuda.memory_reserved(device=device)\n",
    "\n",
    "print(f\"Memory before loading model:\")\n",
    "print(f\"  Allocated: {model_allocated / (1024 ** 3):.2f} GB\")\n",
    "print(f\"  Reserved: {model_reserved / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after loading model:\n",
      "  Allocated: 1.87 GB\n",
      "  Reserved: 2.92 GB\n"
     ]
    }
   ],
   "source": [
    "predictor = build_sam2_video_predictor(CONFIG_PATH, CHKPT_PATH, device=device)\n",
    "\n",
    "model_allocated = torch.cuda.memory_allocated(device=device)\n",
    "model_reserved = torch.cuda.memory_reserved(device=device)\n",
    "\n",
    "print(f\"Memory after loading model:\")\n",
    "print(f\"  Allocated: {model_allocated / (1024 ** 3):.2f} GB\")\n",
    "print(f\"  Reserved: {model_reserved / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG):   4%|▎         | 13/360 [00:00<00:26, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 360/360 [00:11<00:00, 30.94it/s]\n"
     ]
    }
   ],
   "source": [
    "inference_state = predictor.init_state(video_path=FRAMES_PATH, offload_video_to_cpu=True, async_loading_frames=True)\n",
    "predictor.reset_state(inference_state)\n",
    "print(inference_state[\"video_height\"], inference_state[\"video_width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 53\n"
     ]
    }
   ],
   "source": [
    "print(inference_state[\"video_height\"], inference_state[\"video_width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_dimensions(img_dir=FRAMES_PATH):\n",
    "    widths = []\n",
    "    heights = []\n",
    "    \n",
    "    # Iterate over files in the directory\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if filename.lower().endswith(\".jpg\"):\n",
    "            img_path = os.path.join(img_dir, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    widths.append(w)\n",
    "                    heights.append(h)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    \n",
    "    if not widths or not heights:\n",
    "        print(\"No images processed successfully.\")\n",
    "        return\n",
    "    \n",
    "    avg_width = np.mean(widths)\n",
    "    avg_height = np.mean(heights)\n",
    "    min_width = np.min(widths)\n",
    "    max_width = np.max(widths)\n",
    "    min_height = np.min(heights)\n",
    "    max_height = np.max(heights)\n",
    "    \n",
    "    print(\"First width:\", widths[0])\n",
    "    print(\"First height:\", heights[0])\n",
    "    print(\"Average width:\", avg_width)\n",
    "    print(\"Average height:\", avg_height)\n",
    "    print(\"Min width:\", min_width)\n",
    "    print(\"Min height:\", min_height)\n",
    "    print(\"Max width:\", max_width)\n",
    "    print(\"Max height:\", max_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First width: 53\n",
      "First height: 133\n",
      "Average width: 49.28888888888889\n",
      "Average height: 90.30555555555556\n",
      "Min width: 18\n",
      "Min height: 63\n",
      "Max width: 87\n",
      "Max height: 133\n"
     ]
    }
   ],
   "source": [
    "compute_image_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the \"inference state\" dictionary assigns its video_height and video_width according to the dimensions of the *first* frame - i.e. the model assumes all frames in the video are of the same dimension. So we need to pad all frames to the max height and width before feeding to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_directory(base_dir):\n",
    "    max_files = 0\n",
    "    largest_dir = None\n",
    "\n",
    "    # Iterate through each subdirectory in the base directory\n",
    "    for subdir in os.listdir(base_dir):\n",
    "        subdir_path = os.path.join(base_dir, subdir)\n",
    "\n",
    "        # Ensure it's a directory and its name is numeric\n",
    "        if os.path.isdir(subdir_path) and subdir.isdigit():\n",
    "            num_files = len([f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))])\n",
    "\n",
    "            if num_files > max_files:\n",
    "                max_files = num_files\n",
    "                largest_dir = subdir\n",
    "\n",
    "    if largest_dir is not None:\n",
    "        print(f\"Directory with the most files: {largest_dir} ({max_files} files)\")\n",
    "    else:\n",
    "        print(\"No directories with numeric names found.\")\n",
    "\n",
    "# Path to the \"train\" directory\n",
    "dir = \"C:\\\\Users\\\\syeda\\\\OneDrive\\\\Desktop\\\\4th Year\\\\COSC419\\\\jersey-number-pipeline\\\\data\\\\SoccerNet\\\\test\\\\images\"\n",
    "find_largest_directory(dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
